{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical PyTorch: Classifying Names with a Character-Level RNN\n",
    "\n",
    "We will be building and training a basic character-level RNN to classify words. A character-level RNN reads words as a series of characters - outputting a prediction and \"hidden state\" at each step, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to.\n",
    "\n",
    "Specifically, we'll train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling:\n",
    "\n",
    "```\n",
    "$ python predict.py Hinton\n",
    "(-0.47) Scottish\n",
    "(-1.52) English\n",
    "(-3.57) Irish\n",
    "\n",
    "$ python predict.py Schmidhuber\n",
    "(-0.19) German\n",
    "(-2.48) Czech\n",
    "(-2.68) Dutch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Reading\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and understand Tensors:\n",
    "\n",
    "* http://pytorch.org/ For installation instructions\n",
    "* [Deep Learning with PyTorch: A 60-minute Blitz](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) to get started with PyTorch in general\n",
    "* [jcjohnson's PyTorch examples](https://github.com/jcjohnson/pytorch-examples) for an in depth overview\n",
    "* [Introduction to PyTorch for former Torchies](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb) if you are former Lua Torch user\n",
    "\n",
    "It would also be useful to know about RNNs and how they work:\n",
    "\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) shows a bunch of real life examples\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) is about LSTMs specifically but also informative about RNNs in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data\n",
    "\n",
    "Included in the `data/names` directory are 18 text files named as \"[Language].txt\". Each file contains a bunch of names, one name per line, mostly romanized (but we still need to convert from Unicode to ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per language, `{language: [names ...]}`. The generic variables \"category\" and \"line\" (for language and name in our case) are used for later extensibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "all_filenames = glob.glob('dataset/names/*.txt')\n",
    "print(all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicode_to_ascii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "for filename in all_filenames:\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "print('n_categories =', n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have `category_lines`, a dictionary mapping each category (language) to a list of lines (names). We also kept track of `all_categories` (just a list of languages) and `n_categories` for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning Names into Tensors\n",
    "\n",
    "Now that we have all the names organized, we need to turn them into Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we use a \"one-hot vector\" of size `<1 x n_letters>`. A one-hot vector is filled with 0s except for a 1 at index of the current letter, e.g. `\"b\" = <0 1 0 0 0 ...>`.\n",
    "\n",
    "To make a word we join a bunch of those into a 2D matrix `<line_length x 1 x n_letters>`.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in batches - we're just using a batch size of 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letter_to_tensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    letter_index = all_letters.find(letter)\n",
    "    tensor[0][letter_index] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def line_to_tensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        letter_index = all_letters.find(letter)\n",
    "        tensor[li][0][letter_index] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(letter_to_tensor('J'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(line_to_tensor('Jones').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Network\n",
    "\n",
    "Before autograd, creating a recurrent neural network in Torch involved cloning the parameters of a layer over several timesteps. The layers held hidden state and gradients which are now entirely handled by the graph itself. This means you can implement a RNN in a very \"pure\" way, as regular feed-forward layers.\n",
    "\n",
    "This RNN module (mostly copied from [the PyTorch for Torch users tutorial](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb)) is just 2 linear layers which operate on an input and hidden state, with a LogSoftmax layer after the output.\n",
    "\n",
    "![](https://i.imgur.com/Z2xbySO.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually testing the network\n",
    "\n",
    "With our custom `RNN` class defined, we can create a new instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the Tensor for the current letter) and a previous hidden state (which we initialize as zeros at first). We'll get back the output (probability of each language) and a next hidden state (which we keep for the next step).\n",
    "\n",
    "Remember that PyTorch modules operate on Variables rather than straight up Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input = Variable(letter_to_tensor('A'))\n",
    "hidden = rnn.init_hidden()\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)\n",
    "print('output.size =', output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of efficiency we don't want to be creating a new Tensor for every step, so we will use `line_to_tensor` instead of `letter_to_tensor` and use slices. This could be further optimized by pre-computing batches of Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input = Variable(line_to_tensor('Albert'))\n",
    "hidden = Variable(torch.zeros(1, n_hidden))\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is a `<1 x n_categories>` Tensor, where every item is the likelihood of that category (higher is more likely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for Training\n",
    "\n",
    "Before going into training we should make a few helper functions. The first is to interpret the output of the network, which we know to be a likelihood of each category. We can use `Tensor.topk` to get the index of the greatest value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def category_from_output(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(category_from_output(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want a quick way to get a training example (a name and its language):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_training_pair():                                                                                                               \n",
    "    category = random.choice(all_categories)\n",
    "    line = random.choice(category_lines[category])\n",
    "    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "    line_tensor = Variable(line_to_tensor(line))\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = random_training_pair()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Network\n",
    "\n",
    "Now all it takes to train this network is show it a bunch of examples, have it make guesses, and tell it if it's wrong.\n",
    "\n",
    "For the [loss function `nn.NLLLoss`](http://pytorch.org/docs/nn.html#nllloss) is appropriate, since the last layer of the RNN is `nn.LogSoftmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create an \"optimizer\" which updates the parameters of our model according to its gradients. We will use the vanilla SGD algorithm with a low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each loop of training will:\n",
    "\n",
    "* Create input and target tensors\n",
    "* Create a zeroed initial hidden state\n",
    "* Read each letter in and\n",
    "    * Keep hidden state for next letter\n",
    "* Compare final output to target\n",
    "* Back-propagate\n",
    "* Return the output and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(category_tensor, line_tensor):\n",
    "    rnn.zero_grad()\n",
    "    hidden = rnn.init_hidden()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to run that with a bunch of examples. Since the `train` function returns both the output and loss we can print its guesses and also keep track of loss for plotting. Since there are 1000s of examples we print only every `print_every` time steps, and take an average of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_epochs = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Get a random training input and target\n",
    "    category, line, category_tensor, line_tensor = random_training_pair()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "    \n",
    "    # Print epoch number, loss, name and guess\n",
    "    if epoch % print_every == 0:\n",
    "        guess, guess_i = category_from_output(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (epoch, epoch / n_epochs * 100, time_since(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Results\n",
    "\n",
    "Plotting the historical loss from `all_losses` shows the network learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Results\n",
    "\n",
    "To see how well the network performs on different categories, we will create a confusion matrix, indicating for every actual language (rows) which language the network guesses (columns). To calculate the confusion matrix a bunch of samples are run through the network with `evaluate()`, which is the same as `train()` minus the backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.init_hidden()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = random_training_pair()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = category_from_output(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick out bright spots off the main axis that show which languages it guesses incorrectly, e.g. Chinese for Korean, and Spanish for Italian. It seems to do very well with Greek, and very poorly with English (perhaps because of overlap with other languages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    output = evaluate(Variable(line_to_tensor(input_line)))\n",
    "\n",
    "    # Get top N categories\n",
    "    topv, topi = output.data.topk(n_predictions, 1, True)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        value = topv[0][i]\n",
    "        category_index = topi[0][i]\n",
    "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "        predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('Dovesky')\n",
    "predict('Jackson')\n",
    "predict('Satoshi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final versions of the scripts [in the Practical PyTorch repo](https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification) split the above code into a few files:\n",
    "\n",
    "* `data.py` (loads files)\n",
    "* `model.py` (defines the RNN)\n",
    "* `train.py` (runs training)\n",
    "* `predict.py` (runs `predict()` with command line arguments)\n",
    "* `server.py` (serve prediction as a JSON API with bottle.py)\n",
    "\n",
    "Run `train.py` to train and save the network.\n",
    "\n",
    "Run `predict.py` with a name to view predictions: \n",
    "\n",
    "```\n",
    "$ python predict.py Hazaki\n",
    "(-0.42) Japanese\n",
    "(-1.39) Polish\n",
    "(-3.51) Czech\n",
    "```\n",
    "\n",
    "Run `server.py` and visit http://localhost:5533/Yourname to get JSON output of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import argparse\n",
    "import torch.onnx\n",
    "from io import open\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level Language Modeling using RNN and Transformer\n",
    "\n",
    "> This example trains a multi-layer RNN (Elman, GRU, or LSTM) or Transformer on a language modeling task. By default, the training script uses the Wikitext-2 dataset, provided. The trained model can then be used by the generate script to generate new text.\n",
    "\n",
    "```Python\n",
    "python main.py --cuda --epochs 6           # Train a LSTM on Wikitext-2 with CUDA.\n",
    "python main.py --cuda --epochs 6 --tied    # Train a tied LSTM on Wikitext-2 with CUDA.\n",
    "python main.py --cuda --tied               # Train a tied LSTM on Wikitext-2 with CUDA for 40 epochs.\n",
    "python main.py --cuda --epochs 6 --model Transformer --lr 5 # Train a Transformer model on Wikitext-2 with CUDA.\n",
    "python generate.py                         # Generate samples from the default model checkpoint.\n",
    "```\n",
    "\n",
    "This example trains a multi-layer RNN (Elman, GRU, or LSTM) or Transformer on a language modeling task. By default, the training script uses the Wikitext-2 dataset, provided.\n",
    "The trained model can then be used by the generate script to generate new text.\n",
    "\n",
    "```bash\n",
    "python main.py --cuda --epochs 6           # Train a LSTM on Wikitext-2 with CUDA.\n",
    "python main.py --cuda --epochs 6 --tied    # Train a tied LSTM on Wikitext-2 with CUDA.\n",
    "python main.py --cuda --tied               # Train a tied LSTM on Wikitext-2 with CUDA for 40 epochs.\n",
    "python main.py --cuda --epochs 6 --model Transformer --lr 5\n",
    "                                           # Train a Transformer model on Wikitext-2 with CUDA.\n",
    "\n",
    "python generate.py                         # Generate samples from the default model checkpoint.\n",
    "```\n",
    "\n",
    "The model uses the `nn.RNN` module (and its sister modules `nn.GRU` and `nn.LSTM`) or Transformer module (`nn.TransformerEncoder` and `nn.TransformerEncoderLayer`) which will automatically use the cuDNN backend if run on CUDA with cuDNN installed.\n",
    "\n",
    "During training, if a keyboard interrupt (Ctrl-C) is received, training is stopped and the current model is evaluated against the test dataset.\n",
    "\n",
    "The `main.py` script accepts the following arguments:\n",
    "\n",
    "```bash\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --data DATA           location of the data corpus\n",
    "  --model MODEL         type of network (RNN_TANH, RNN_RELU, LSTM, GRU, Transformer)\n",
    "  --emsize EMSIZE       size of word embeddings\n",
    "  --nhid NHID           number of hidden units per layer\n",
    "  --nlayers NLAYERS     number of layers\n",
    "  --lr LR               initial learning rate\n",
    "  --clip CLIP           gradient clipping\n",
    "  --epochs EPOCHS       upper epoch limit\n",
    "  --batch_size N        batch size\n",
    "  --bptt BPTT           sequence length\n",
    "  --dropout DROPOUT     dropout applied to layers (0 = no dropout)\n",
    "  --tied                tie the word embedding and softmax weights\n",
    "  --seed SEED           random seed\n",
    "  --cuda                use CUDA\n",
    "  --mps                 enable GPU on macOS\n",
    "  --log-interval N      report interval\n",
    "  --save SAVE           path to save the final model\n",
    "  --onnx-export ONNX_EXPORT\n",
    "                        path to export the final model in onnx format\n",
    "  --nhead NHEAD         the number of heads in the encoder/decoder of the transformer model\n",
    "  --dry-run             verify the code and the model\n",
    "```\n",
    "\n",
    "With these arguments, a variety of models can be tested.\n",
    "As an example, the following arguments produce slower but better models:\n",
    "\n",
    "```bash\n",
    "python main.py --cuda --emsize 650 --nhid 650 --dropout 0.5 --epochs 40\n",
    "python main.py --cuda --emsize 650 --nhid 650 --dropout 0.5 --epochs 40 --tied\n",
    "python main.py --cuda --emsize 1500 --nhid 1500 --dropout 0.65 --epochs 40\n",
    "python main.py --cuda --emsize 1500 --nhid 1500 --dropout 0.65 --epochs 40 --tied\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 167.99 | loss  7.67 | ppl  2133.09\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 115.96 | loss  6.88 | ppl   969.98\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 120.61 | loss  6.55 | ppl   697.50\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 118.72 | loss  6.38 | ppl   590.38\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 118.87 | loss  6.22 | ppl   504.97\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 123.17 | loss  6.14 | ppl   465.82\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 123.21 | loss  6.03 | ppl   415.86\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 133.36 | loss  6.03 | ppl   415.57\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 119.22 | loss  5.90 | ppl   363.44\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 125.32 | loss  5.88 | ppl   358.00\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 125.53 | loss  5.77 | ppl   320.56\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 123.45 | loss  5.78 | ppl   324.66\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 116.02 | loss  5.77 | ppl   319.73\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 115.53 | loss  5.65 | ppl   284.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 386.90s | valid loss  7.10 | valid ppl  1211.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 116.30 | loss  5.65 | ppl   284.08\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 115.99 | loss  5.63 | ppl   279.96\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 113.89 | loss  5.48 | ppl   240.30\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 114.10 | loss  5.50 | ppl   244.35\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 126.31 | loss  5.48 | ppl   238.67\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 126.96 | loss  5.46 | ppl   234.96\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 132.34 | loss  5.45 | ppl   232.91\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 135.01 | loss  5.51 | ppl   246.73\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 130.12 | loss  5.40 | ppl   220.42\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 123.05 | loss  5.41 | ppl   223.28\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 123.95 | loss  5.31 | ppl   203.11\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 135.29 | loss  5.35 | ppl   210.67\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 141.90 | loss  5.37 | ppl   214.22\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 147.24 | loss  5.27 | ppl   194.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 393.06s | valid loss  7.02 | valid ppl  1119.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 116.28 | loss  5.32 | ppl   205.34\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 125.90 | loss  5.34 | ppl   208.70\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 132.37 | loss  5.18 | ppl   178.16\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 125.94 | loss  5.22 | ppl   185.82\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 131.50 | loss  5.22 | ppl   184.08\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 147.37 | loss  5.22 | ppl   184.73\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 123.96 | loss  5.22 | ppl   185.42\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 139.38 | loss  5.30 | ppl   199.35\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 152.09 | loss  5.18 | ppl   178.51\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 140.16 | loss  5.20 | ppl   181.80\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 127.56 | loss  5.11 | ppl   165.49\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 126.20 | loss  5.16 | ppl   174.38\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 128.61 | loss  5.18 | ppl   177.38\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 148.03 | loss  5.09 | ppl   162.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 416.53s | valid loss  7.07 | valid ppl  1178.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 130.56 | loss  5.20 | ppl   180.70\n",
      "| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 137.12 | loss  5.19 | ppl   179.54\n",
      "| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 134.01 | loss  5.02 | ppl   151.33\n",
      "| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 134.09 | loss  5.07 | ppl   159.18\n",
      "| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 143.00 | loss  5.03 | ppl   153.26\n",
      "| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 119.07 | loss  5.02 | ppl   151.80\n",
      "| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 142.64 | loss  5.02 | ppl   151.43\n",
      "| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 130.86 | loss  5.07 | ppl   159.54\n",
      "| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 122.36 | loss  4.96 | ppl   142.49\n",
      "| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 118.27 | loss  4.97 | ppl   144.15\n",
      "| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 113.66 | loss  4.86 | ppl   129.12\n",
      "| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 113.37 | loss  4.89 | ppl   132.61\n",
      "| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 114.02 | loss  4.90 | ppl   134.43\n",
      "| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 113.07 | loss  4.80 | ppl   121.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 388.74s | valid loss  6.65 | valid ppl   771.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 117.80 | loss  5.04 | ppl   153.78\n",
      "| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 115.62 | loss  5.06 | ppl   157.37\n",
      "| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 114.03 | loss  4.89 | ppl   132.75\n",
      "| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 114.11 | loss  4.95 | ppl   141.02\n",
      "| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 114.47 | loss  4.92 | ppl   137.63\n",
      "| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 115.07 | loss  4.93 | ppl   138.03\n",
      "| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 113.66 | loss  4.94 | ppl   139.48\n",
      "| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 115.97 | loss  4.99 | ppl   147.27\n",
      "| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 114.72 | loss  4.89 | ppl   132.62\n",
      "| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 114.07 | loss  4.91 | ppl   135.27\n",
      "| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 113.48 | loss  4.80 | ppl   122.10\n",
      "| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 116.79 | loss  4.84 | ppl   126.25\n",
      "| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 115.17 | loss  4.85 | ppl   128.23\n",
      "| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 116.09 | loss  4.77 | ppl   117.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 357.44s | valid loss  6.62 | valid ppl   751.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 118.16 | loss  4.97 | ppl   143.91\n",
      "| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 112.74 | loss  4.99 | ppl   147.03\n",
      "| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 129.93 | loss  4.82 | ppl   123.86\n",
      "| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 125.24 | loss  4.88 | ppl   132.28\n",
      "| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 119.51 | loss  4.86 | ppl   129.19\n",
      "| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 111.95 | loss  4.87 | ppl   130.52\n",
      "| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 113.07 | loss  4.88 | ppl   132.27\n",
      "| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 112.16 | loss  4.95 | ppl   140.73\n",
      "| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 112.76 | loss  4.84 | ppl   126.55\n",
      "| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 111.85 | loss  4.86 | ppl   128.78\n",
      "| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 110.77 | loss  4.76 | ppl   116.98\n",
      "| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 118.15 | loss  4.80 | ppl   121.39\n",
      "| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 121.56 | loss  4.82 | ppl   123.68\n",
      "| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 110.35 | loss  4.73 | ppl   113.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 360.66s | valid loss  6.66 | valid ppl   782.05\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baojian/Library/Mobile Documents/com~apple~CloudDocs/fudan/2024/NLP-24/L07-RNNs-LSTM/demo-code/main.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  6.46 | test ppl   638.96\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# say you do NOT have a GPU\n",
    "%run main.py --mps --epochs 6 --data './dataset/wikitext-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You have mps device, to enable macOS GPU run with --mps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baojian/Library/Mobile Documents/com~apple~CloudDocs/fudan/2024/NLP-24/L07-RNNs-LSTM/demo-code/generate.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(f, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 0/1000 words\n",
      "| Generated 100/1000 words\n",
      "| Generated 200/1000 words\n",
      "| Generated 300/1000 words\n",
      "| Generated 400/1000 words\n",
      "| Generated 500/1000 words\n",
      "| Generated 600/1000 words\n",
      "| Generated 700/1000 words\n",
      "| Generated 800/1000 words\n",
      "| Generated 900/1000 words\n"
     ]
    }
   ],
   "source": [
    "# Generate samples from the default model checkpoint.\n",
    "%run generate.py --data './dataset/wikitext-2' --checkpoint './model-cuda.pt' --outf 'generated.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to the restoration of stabilized and tamp a few place : <eos> In 253 , what was good architectural responses\n",
      "as known as <unk> or <unk> and more under the entire musical events that survives at projects or to .\n",
      "They still were a different range of chemistry . By front music that erected all total lines of remains of\n",
      "the planet 's relatively thief , <unk> reporting the water . <eos> In 2007 , Crosby mantelli , \" poverty\n",
      "\" and \" an outstanding Star Dumpty \" . Before the academic Sentinel understood a lengthy decrease against a firm\n",
      "against assimilated @-@ Caves , he was also categorized as Rome 's son , in Fort Noble , Kesteven ,\n",
      "Henan , Metro , and Arthur . These institutions searching the Adummatu to \" overcome himself - \" and Thom\n",
      "Francis William Track which received . They headed to the Muslim Prison Douglas to prepare he had implemented his Chucky\n",
      ", a man wanting to teach no use of multiple games , Gary , <unk> , and <unk> @-@ like\n",
      "row . Palaeoscincus , highlighting by turning out , civilian and political reflection , that event was ruled that \"\n",
      "it should be able to look to write tradition that would be Mason Stakes . \" <eos> The featuring response\n",
      "to him from <unk> Young issued the name a collection of amateur , very contemporary Often and languages approximately considerable\n",
      "24 years as his character was a planet . Two seaside legislators were both shown Tintin Baku , for instance\n",
      "and restaurants . One of gross would be permitted in China on the time of 1823 . The outlook in\n",
      "numerous forest Wehrmacht began publishing pounds ( Lifestream number of Crusher , despite a small pessimistic product , \" Allāh\n",
      "\" remains of university station ) , which was also ordered to reconcile no quality pillars . It was the\n",
      "first time in Russ Arthur Rubinstein . <eos> <eos> = = Cast = = <eos> <eos> Walpole was a heightened\n",
      "author of John Camp Strange Marshal and imprisoned from 1629 ) . He had Lawrence <unk> one of the rabbi\n",
      "and thirty @-@ critical works in his expenses . The main kings of Central America was annexed by British native\n",
      "protests across March for sexual relations began entirely to so . Vice described in <unk> and John , was eventually\n",
      "of modern tombs on architecture on June 4 , 1964 . On their own <unk> Wayback ( <unk> ) and\n",
      "appraisal 's for a year he became killed by a Assi Airlines , 454 , and Cork are said to\n",
      "be reduced money , and is referred to by Johann v. <unk> noted that this actresses did not have been\n",
      "<unk> in Italy , although he on his return and stay them across it was \" ulama to clear to\n",
      "doubt or being <unk> \" . <eos> According to Giger converted into VT ( 2010 ) Secretary of 1885 ,\n",
      "Günther critics that speculate that <unk> bore through Hairan I is <unk> up Peshkin , despite the name 's son\n",
      ". It takes advantage for what he would join by a <unk> he declined to support his father or Jeremy\n",
      ", who may have differed after Richard for <unk> ( 1896 . greyhound <unk> ) <unk> by a lieutenant .\n",
      "<unk> power from Earnest disintegration objected to his wife , Zenobia . <unk> , <unk> for <unk> , has compromised\n",
      "and concealing a willingness mortar range . The couple are also coined . <eos> O 'Malley perfect places children ,\n",
      "including by Elrane unacceptable in the <unk> Millennium Centre . It remained ballet , by which overshadow 1717 , married\n",
      "<unk> <unk> lung Greenwood , diluted by Wren Sri <unk> and Meteor . Since they appear to be <unk> in\n",
      "unemployment , O 'Malley remnants in 1952 still over events , and his biographer Thomas <unk> had joined Richardson in\n",
      "two four years . Byung @-@ <unk> followed with first Baltimore meanwhile run after stint a conflict on Little ,\n",
      "supported by khani . He admitted , \" Ten courts might be certainly questioned in Lydney <unk> <unk> , <unk>\n",
      ", to take a natural inflict support against it in a and fan . He finds nothing to try .\n",
      "<eos> He his initial Des Tempo , son of Hebei <eos> On the \" dads \" to You . <eos>\n",
      "Turkey was the youngest memory of all and sports in both trades , by those of angels ; hanged ,\n",
      "Aspects and <unk> ( as twenty @-@ eight touch for a child when they use \" a compact flesh of\n",
      "some period from Alexandra Moines ( Eva ) \" instead or ( \" king as dead ) , \" is\n",
      "a pocket octagonal stallion , published a team called remained in Palmyra Journal and <unk> against <unk> . <eos> Henry\n",
      "Leonard Price was assessed using artefacts to select his last American events and that in 1898 , Walpole possessed up\n",
      "to some Nambu themes and <unk> . During his falsetto correspondence , new younger Semitic forces were introduced negative as\n",
      "most active pieces of his short brother and contributed ; this could have been regularly again dealt since June 1942\n",
      ". He said \" undoubtedly legally 1135 , but none who gets the highway for the day . \" Mosley\n",
      "commented \" Eaton was tempered by deaths for him that he could bring them to evidence work and tendency under\n",
      "ostrich [ sic ] under me people and d Scribe . \" O 'Malley received a military job in Chief\n",
      "in 1892 , which he replied for his memoirs . <eos> Other accounts were published alongside heel Mosley . <eos>\n",
      "<eos> <eos> = Chapter I Love : The Dirt follows you Do creating You ( / ernst ) ? )\n",
      "<eos> \" The Outsider ' ( 2006 ) that flows before cross @-@ border @-@ naked apartment ( Le al\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat generated.txt"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
