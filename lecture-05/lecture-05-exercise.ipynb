{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6388403c",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Pytorch Tutorial</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f8281f-9091-40c4-86fd-841dc701f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78daf6e-1705-4899-bc2f-db59942a4c7b",
   "metadata": {},
   "source": [
    "## 1 PyTorch Introduction\n",
    "\n",
    "The following tutorial is adopted from:\n",
    "\n",
    "https://brsoff.github.io/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "https://github.com/yunjey/pytorch-tutorial/\n",
    "\n",
    "**WHAT IS PYTORCH?**\n",
    "\n",
    ">It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "> 1. A replacement for NumPy to use the power of GPUs\n",
    "> 2. a deep learning research platform that provides maximum flexibility and speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe8016-c2dd-48cb-9d98-9aec6b4e6707",
   "metadata": {},
   "source": [
    "### **1.1 Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937bff62-fada-4bf4-bb19-e9df090dd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6517ed-69df-4dbf-801c-d6f002e65c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-01 Construct a uninitialized 5x3 matrix with random values (depending on memory state):\n",
    "\n",
    "x = torch.empty(5, 4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5b307-d9a9-4854-86f7-b10a31387d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-02 Construct a randomly initialized matrix sampled from a uniform distribution U(0,1):\n",
    "\n",
    "x = torch.rand(5,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c938fb-ab14-407e-b293-4aae5ad08e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-03 Construct a matrix filled zeros and of dtype long:\n",
    "\n",
    "x = torch.zeros(5, 4, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b36e1-5583-451c-af8e-33b75f3c2f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-04 Construct a tensor directly from data:\n",
    "\n",
    "x = torch.tensor([5.5, 3, 4., -1.])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163dee65-af10-4fba-b99a-bbe321539a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-05 These methods will reuse properties of the \n",
    "# input tensor, e.g. dtype, unless new values are provided by user\n",
    "\n",
    "x = x.new_ones(5, 4, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "print(type(x))\n",
    "\n",
    "# Creates a new tensor with the same size as x, filled with random numbers from a normal distribution (mean=0, std=1), and converts to float32 type\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c2cc8-8ff0-4d97-a893-029517a38004",
   "metadata": {},
   "source": [
    "### **1.2 Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512e3f2-2afe-4f38-b5fc-dca1ba0167da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-06 Get its size:\n",
    "\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74280f6-b6fb-4a4e-9dcb-01f8f4d5a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-07 Addition using +\n",
    "\n",
    "y = torch.rand(5, 4)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0165d-c875-44a0-b6ab-7fe79eebe6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-08 Addition using add() method\n",
    "\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277133f-f432-4133-9b27-49f8f0d2ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-09 Addition: providing an output tensor as argument\n",
    "\n",
    "result = torch.empty(5, 4)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff547ce9-0a00-407b-a253-f6cca7206a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-10 Addition: in-place\n",
    "# Any operation that mutates a tensor in-place is post-fixed with an _. \n",
    "# For example: x.copy_(y), x.t_(), will change x.\n",
    "\n",
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf753a39-3627-4a86-b9b7-97084dcc4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-11 You can use standard NumPy-like indexing with all bells and whistles!\n",
    "\n",
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd22173-4c17-4b35-8865-9888256281db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-12 Resizing: If you want to resize/reshape tensor, you can use torch.view or torch.reshape.\n",
    "\n",
    "# torch.view:\n",
    "# Reshapes the tensor without changing its data.\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())\n",
    "print(f\"is x, y, z share memory? {x.data_ptr() == y.data_ptr() == z.data_ptr()}\")\n",
    "\n",
    "# torch.reshape:\n",
    "a = torch.randn(4, 4)\n",
    "b = a.reshape(16)\n",
    "c = a.reshape(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(a.size(), b.size(), c.size())\n",
    "print(f\"is a, b, c share memory? {a.data_ptr() == b.data_ptr() == c.data_ptr()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ad5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the difference between reshape and view\n",
    "x = torch.randn(4, 4)\n",
    "x_t = x.t()  # Transpose operation makes memory non-contiguous\n",
    "\n",
    "# Using view will raise an error\n",
    "try:\n",
    "    y = x_t.view(16)\n",
    "except RuntimeError as e:\n",
    "    print(\"view failed:\", e)\n",
    "\n",
    "# Using reshape succeeds\n",
    "z = x_t.reshape(16)\n",
    "print(\"\\nreshape succeeded:\")\n",
    "print(z)\n",
    "\n",
    "# Check memory contiguity\n",
    "print(\"\\nMemory contiguity check:\")\n",
    "print(f\"Is x_t contiguous: {x_t.is_contiguous()}\")\n",
    "print(f\"Is z contiguous: {z.is_contiguous()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6a38d-64b1-4e4e-a331-d82eae1b6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-13 Get numerical value: \n",
    "# If you have a one element tensor, use .item() \n",
    "# to get the value as a Python number\n",
    "\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "print(y[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f7a39-07ce-4a9a-a598-7d48be10f645",
   "metadata": {},
   "source": [
    "### **1.3 Converting a Torch Tensor to a NumPy Array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd8f72-286b-41fe-8e36-5d63c9e74902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-14 Converting a Torch Tensor to a NumpPy Array\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc4edb-7ab1-4c9c-aa81-2397f3636a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-15 See how the numpy array changed in value.\n",
    "\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# They share the same memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e349046-696c-4fc5-b845-7e5aeddc208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-16 Converting NumPy Array to Torch Tensor\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0cba90-649c-4292-aaad-ad7fad2994b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-17 Tensors can be moved onto any device using the .to method.\n",
    "\n",
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    print(f\"x's device: {x.device}\")\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    print(f\"x's device: {x.device}\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091e1e7-86f2-4326-9e03-bb3d30e7f8cf",
   "metadata": {},
   "source": [
    "### **1.4 Autograd: automatic differentiation**\n",
    "\n",
    "Central to all neural networks in PyTorch is the *autograd* package. Let’s first briefly visit this, and we will then go to training our first neural network.\n",
    "\n",
    "The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fdeac1-92d4-4389-b6bc-87ecb508e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-18 Create a tensor and set requires_grad=True to track computation with it\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60584f-9460-4133-b2f4-11427ab32b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-19 do operations on y\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)\n",
    "\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25bfab-a84d-42eb-8404-7e73f5435f0d",
   "metadata": {},
   "source": [
    "### **1.5 Gradients**\n",
    "\n",
    "Let’s backprop now Because out contains a single scalar, \n",
    "out.backward() is equivalent to out.backward(torch.tensor(1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1696ff-1966-4bff-a601-0eae2a329f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-20 do the backprop\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b66f0-9dca-459f-8864-e8bf7b17b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-21 the autograd operation could complicated\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)\n",
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebafd11e-533a-4b2e-81dc-5e59f9427a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-22 Use requires_grad()\n",
    "# You can also stop autograd from tracking history on Tensors with \n",
    "# .requires_grad=True by wrapping the code block in with torch.no_grad():\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf481ba",
   "metadata": {},
   "source": [
    "### **1.6 Basic autograd example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-23 create a neural network model\n",
    "\n",
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)\n",
    "\n",
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# 1-step gradient descent.\n",
    "optimizer.step()\n",
    "\n",
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83b7ca",
   "metadata": {},
   "source": [
    "An typical example of PyTorch neural network training workflow\n",
    "\n",
    "1. Data Preparation\n",
    "2. Model Definition\n",
    "    - Create neural network layers (e.g., `nn.Linear` )\n",
    "    - Initialize model parameters (weights and biases)\n",
    "3. Loss Function & Optimizer Setup\n",
    "    - Choose a loss function (e.g., `MSELoss` )\n",
    "    - Select an optimizer (e.g., `SGD` )\n",
    "    - Set hyperparameters (e.g., learning rate)\n",
    "4. Training Loop\n",
    "    - Forward Pass\n",
    "    - Loss Computation\n",
    "    - Backward Pass\n",
    "        - Clear previous gradients  \n",
    "        - Compute gradients using `backward()`\n",
    "    - Parameter Update:\n",
    "        - Update weights using optimizer's `step()`  \n",
    "5. Evaluation\n",
    "    - Switch to evaluation mode\n",
    "    - Make predictions on validation set\n",
    "    - Monitor validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51243947",
   "metadata": {},
   "source": [
    "### **1.7 Datasets**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should build your custom dataset as below.\n",
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        # 1. Initialize file paths or a list of file names. \n",
    "        # self.data = ... # initialize a list of file names.\n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        # return self.data[index] # return a data pair\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 1 to the total size of your dataset.\n",
    "        # return len(self.data) # return the total size of your dataset\n",
    "        return 1\n",
    "\n",
    "# You can then use the prebuilt data loader. \n",
    "custom_dataset = CustomDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72695a",
   "metadata": {},
   "source": [
    "### **1.8 Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e3d5e-177a-46de-8a5d-92cc74561773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural network model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5cea0-ad72-489b-a2c2-cfb22a6fdcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-24 The learnable parameters of a model are returned by net.parameters()\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # size of conv1's .weight\n",
    "print(net.conv1.weight, net.conv1.bias) # conv1's.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa334c1-da04-4fc4-856f-0728feadc3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-25 forward pass \n",
    "input = torch.randn(1, 3, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9316fe11-9cb2-49c5-a712-8e7e254c40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-26 Zero the gradient buffers of all parameters and backprops with random gradients:\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10)) # backward with random gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e3c05d",
   "metadata": {},
   "source": [
    "### **1.9 Loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cadff9-a14b-4703-92d9-6626854f7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-27 Loss function \n",
    "\n",
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "\n",
    "# Task-28 Backprop\n",
    "# computational graph of backpropagation\n",
    "# MSELoss -> Linear -> ReLU \n",
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db82c20",
   "metadata": {},
   "source": [
    "### **1.10 Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea164b-25c1-4751-a782-393dcd126b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-28 Backprop\n",
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)\n",
    "# Update the weights\n",
    "\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7935a",
   "metadata": {},
   "source": [
    "### **1.11 Training code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c826a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94917551-cd05-4340-9537-625465a81164",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(-2.0, requires_grad=True)\n",
    "y = torch.tensor(5.0, requires_grad=True)\n",
    "z = torch.tensor(-4.0, requires_grad=True)\n",
    "f = (x+y)*z # Define the computation graph\n",
    "f.backward() # PyTorch’s internal backward gradient computation\n",
    "print('Gradients after backpropagation:', x.grad, y.grad, z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9a92d",
   "metadata": {},
   "source": [
    "### **1.12 A Full Pipeline for Practical Neural Network Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and construct CIFAR-10 dataset.\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                            train=False,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            download=True)                                        \n",
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)\n",
    "\n",
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=False)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47883f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Model.\n",
    "net = Net()\n",
    "\n",
    "# Loss and Optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Actual usage of the data loader is as below.\n",
    "pbar = tqdm(train_loader, ncols=100, position=0, leave=True)\n",
    "for batch_idx, (images, labels) in enumerate(pbar):\n",
    "    # Forward pass\n",
    "    outputs = net(images)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    loss.backward()       # Compute gradients\n",
    "    optimizer.step()      # Update weights\n",
    "    \n",
    "    if batch_idx % 10 == 0:\n",
    "        pbar.set_description(f'Training Loss: {loss.item():.4f}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "# Since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)     \n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model \n",
    "# Download and load the pretrained ResNet-18.\n",
    "resnet = torchvision.models.resnet18(weights=True)\n",
    "# If you want to finetune only the top layer of the model, set as below.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the top layer for finetuning.\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 10)  # 10 is an example.\n",
    "\n",
    "# Forward pass.\n",
    "images = torch.randn(64, 3, 224, 224)\n",
    "outputs = resnet(images)\n",
    "print(outputs.size())     # (64, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load the model\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt', weights_only=False)\n",
    "\n",
    "# Save and load only the model parameters (recommended).\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345ed9f-f01e-4d8e-92a7-cb4fa568576f",
   "metadata": {},
   "source": [
    "## 2 Micrograd Package\n",
    "This section introduces the Micrograd package, which is a minimal implementation of an autograd engine (automatic differentiation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1d4d2",
   "metadata": {},
   "source": [
    "### **2.1 `Value` class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7cd761-bb8f-48c5-b168-e036a72e90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f3761b",
   "metadata": {},
   "source": [
    "### **2.2 `Module`, `Neuron`, `Layer` and `MLP` classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158992e-14cd-4c3c-9008-f07c90fb3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Base class for all neural network modules\n",
    "# Provides basic functionality for parameter management and gradient zeroing\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# Single neuron class that implements either a ReLU or Linear neuron\n",
    "# Inherits from Module base class for parameter management\n",
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(0)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        return act.relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
    "\n",
    "# Neural network layer class that contains multiple neurons\n",
    "# Inherits from Module base class for parameter management\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "# Multi-Layer Perceptron (MLP) class that implements a feedforward neural network\n",
    "class MLP(Module):\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20802d08-1257-4036-9541-a809f45ff091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def test_sanity_check():\n",
    "    \"\"\"\n",
    "    A basic test to verify the implementation of autograd matches PyTorch's behavior.\n",
    "    Compares the results of forward and backward passes between custom Value class\n",
    "    and PyTorch's autograd.\n",
    "    \"\"\"\n",
    "    x = Value(-4.0)\n",
    "    z = 2 * x + 2 + x\n",
    "    q = z.relu() + z * x\n",
    "    h = (z * z).relu()\n",
    "    y = h + q + q * x\n",
    "    y.backward()\n",
    "    xmg, ymg = x, y\n",
    "\n",
    "    x = torch.Tensor([-4.0]).double()\n",
    "    x.requires_grad = True\n",
    "    z = 2 * x + 2 + x\n",
    "    q = z.relu() + z * x\n",
    "    h = (z * z).relu()\n",
    "    y = h + q + q * x\n",
    "    y.backward()\n",
    "    xpt, ypt = x, y\n",
    "\n",
    "    # forward pass went well\n",
    "    assert ymg.data == ypt.data.item()\n",
    "    # backward pass went well\n",
    "    assert xmg.grad == xpt.grad.item()\n",
    "\n",
    "def test_more_ops():\n",
    "    \"\"\"\n",
    "    A more comprehensive test comparing custom autograd implementation with PyTorch.\n",
    "    Tests various operations including addition, multiplication, power, ReLU,\n",
    "    and division.\n",
    "    \"\"\"\n",
    "    a = Value(-4.0)\n",
    "    b = Value(2.0)\n",
    "    c = a + b\n",
    "    d = a * b + b**3\n",
    "    c += c + 1\n",
    "    c += 1 + c + (-a)\n",
    "    d += d * 2 + (b + a).relu()\n",
    "    d += 3 * d + (b - a).relu()\n",
    "    e = c - d\n",
    "    f = e**2\n",
    "    g = f / 2.0\n",
    "    g += 10.0 / f\n",
    "    g.backward()\n",
    "    amg, bmg, gmg = a, b, g\n",
    "\n",
    "    a = torch.Tensor([-4.0]).double()\n",
    "    b = torch.Tensor([2.0]).double()\n",
    "    a.requires_grad = True\n",
    "    b.requires_grad = True\n",
    "    c = a + b\n",
    "    d = a * b + b**3\n",
    "    c = c + c + 1\n",
    "    c = c + 1 + c + (-a)\n",
    "    d = d + d * 2 + (b + a).relu()\n",
    "    d = d + 3 * d + (b - a).relu()\n",
    "    e = c - d\n",
    "    f = e**2\n",
    "    g = f / 2.0\n",
    "    g = g + 10.0 / f\n",
    "    g.backward()\n",
    "    apt, bpt, gpt = a, b, g\n",
    "\n",
    "    tol = 1e-6\n",
    "    # forward pass went well\n",
    "    assert abs(gmg.data - gpt.data.item()) < tol\n",
    "    # backward pass went well\n",
    "    assert abs(amg.grad - apt.grad.item()) < tol\n",
    "    assert abs(bmg.grad - bpt.grad.item()) < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f62254c-70fe-4cc5-8c56-930638e75d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sanity_check()\n",
    "test_more_ops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a1246-7833-4d4f-957f-ec62c6bd64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(-4.0)\n",
    "b = Value(2.0)\n",
    "c = a + b\n",
    "d = a * b + b**3\n",
    "c += c + 1\n",
    "c += 1 + c + (-a)\n",
    "d += d * 2 + (b + a).relu()\n",
    "d += 3 * d + (b - a).relu()\n",
    "e = c - d\n",
    "f = e**2\n",
    "g = f / 2.0\n",
    "g += 10.0 / f\n",
    "print(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass\n",
    "g.backward()\n",
    "print(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da\n",
    "print(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
