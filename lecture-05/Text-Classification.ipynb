{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Text Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification is one of the basic and most important task of Natural Language Processing. In this notebook, I am focussing on Sentiment Analysis task using CNN model.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "[Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Vocabulary: 251639\n",
      "Vector size of Text Vocabulary:  torch.Size([251639, 300])\n",
      "Label Length: 2\n",
      "Number of samples in each class:\n",
      "Class pos: 12500 samples\n",
      "Class neg: 12500 samples\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(test_sen=None):\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer : Breaks sentences into a list of words. If sequential=False, no tokenization is applied\n",
    "    Field : A class that stores information about the way of preprocessing\n",
    "    fix_length : An important property of TorchText is that we can let the input to be variable length, and TorchText will\n",
    "                 dynamically pad each sequence to the longest sequence in that \"batch\". But here we are using fi_length which\n",
    "                 will pad each sequence to have a fix length of 200.\n",
    "                 \n",
    "    build_vocab : It will first make a vocabulary or dictionary mapping all the unique words present in the train_data to an\n",
    "                  idx and then after it will use GloVe word embedding to map the index to the corresponding word embedding.\n",
    "                  \n",
    "    vocab.vectors : This returns a torch tensor of shape (vocab_size x embedding_dim) containing the pre-trained word embeddings.\n",
    "    BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Define tokenizer function to split text into words\n",
    "    tokenize = lambda x: x.split()\n",
    "    # Create text field processor with configurations:\n",
    "    # - sequential: Process as sequence data\n",
    "    # - tokenize: Use the defined tokenizer\n",
    "    # - lower: Convert to lowercase\n",
    "    # - include_lengths: Keep sequence length info\n",
    "    # - batch_first: Put batch dimension first\n",
    "    # - fix_length: Pad all sequences to 200 words\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)\n",
    "    # Create label field processor for sentiment labels (0/1)\n",
    "    LABEL = data.LabelField(dtype=torch.long)\n",
    "    # Load IMDB dataset and split into train/test sets\n",
    "    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    # Build vocabulary using training data and load GloVe embeddings\n",
    "    TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "\n",
    "    word_embeddings = TEXT.vocab.vectors\n",
    "    print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
    "    print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
    "    print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
    "    \n",
    "    # Print number of samples for each label\n",
    "    print(\"Number of samples in each class:\")\n",
    "    for label, count in LABEL.vocab.freqs.items():\n",
    "        print(f\"Class {label}: {count} samples\")\n",
    "\n",
    "    train_data, valid_data = train_data.split() # Further splitting of training_data to create new training_data & validation_data\n",
    "    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=32, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
    "\n",
    "    '''Alternatively we can also use the default configurations'''\n",
    "    # train_iter, test_iter = datasets.IMDB.iters(batch_size=32)\n",
    "\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter\n",
    "\n",
    "# Load the dataset\n",
    "# We use pre-trained word embeddings as weights for our CNN\n",
    "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2 CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\tdef __init__(self, batch_size, output_size, in_channels, out_channels, kernel_heights, stride, padding, keep_probab, vocab_size, embedding_length, weights):\n",
    "\t\tsuper(CNN, self).__init__()\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\tArguments\n",
    "\t\t---------\n",
    "\t\tbatch_size : Size of each batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "\t\toutput_size : 2 = (pos, neg)\n",
    "\t\tin_channels : Number of input channels. Here it is 1 as the input data has dimension = (batch_size, num_seq, embedding_length)\n",
    "\t\tout_channels : Number of output channels after convolution operation performed on the input matrix\n",
    "\t\tkernel_heights : A list consisting of 3 different kernel_heights. Convolution will be performed 3 times and finally results from each kernel_height will be concatenated.\n",
    "\t\tkeep_probab : Probability of retaining an activation node during dropout operation\n",
    "\t\tvocab_size : Size of the vocabulary containing unique words\n",
    "\t\tembedding_length : Embedding dimension of GloVe word embeddings\n",
    "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n",
    "\t\t--------\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.output_size = output_size\n",
    "\t\tself.in_channels = in_channels\n",
    "\t\tself.out_channels = out_channels\n",
    "\t\tself.kernel_heights = kernel_heights\n",
    "\t\tself.stride = stride\n",
    "\t\tself.padding = padding\n",
    "\t\tself.vocab_size = vocab_size\n",
    "\t\tself.embedding_length = embedding_length\n",
    "\t\t\n",
    "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
    "\t\tself.conv1 = nn.Conv2d(in_channels, out_channels, (kernel_heights[0], embedding_length), stride, padding)\n",
    "\t\tself.conv2 = nn.Conv2d(in_channels, out_channels, (kernel_heights[1], embedding_length), stride, padding)\n",
    "\t\tself.conv3 = nn.Conv2d(in_channels, out_channels, (kernel_heights[2], embedding_length), stride, padding)\n",
    "\t\tself.dropout = nn.Dropout(keep_probab)\n",
    "\t\tself.label = nn.Linear(len(kernel_heights)*out_channels, output_size)\n",
    "\t\n",
    "\tdef conv_block(self, input, conv_layer):\n",
    "\t\tconv_out = conv_layer(input)# conv_out.size() = (batch_size, out_channels, dim, 1)\n",
    "\t\tactivation = F.relu(conv_out.squeeze(3))# activation.size() = (batch_size, out_channels, dim1)\n",
    "\t\tmax_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)# maxpool_out.size() = (batch_size, out_channels)\n",
    "\t\t\n",
    "\t\treturn max_out\n",
    "\t\n",
    "\tdef forward(self, input_sentences, batch_size=None):\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\tThe idea of the Convolutional Neural Netwok for Text Classification is very simple. We perform convolution operation on the embedding matrix \n",
    "\t\twhose shape for each batch is (num_seq, embedding_length) with kernel of varying height but constant width which is same as the embedding_length.\n",
    "\t\tWe will be using ReLU activation after the convolution operation and then for each kernel height, we will use max_pool operation on each tensor \n",
    "\t\tand will filter all the maximum activation for every channel and then we will concatenate the resulting tensors. This output is then fully connected\n",
    "\t\tto the output layers consisting two units which basically gives us the logits for both positive and negative classes.\n",
    "\t\t\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tinput_sentences: input_sentences of shape = (batch_size, num_sequences)\n",
    "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "\t\t\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tOutput of the linear layer containing logits for pos & neg class.\n",
    "\t\tlogits.size() = (batch_size, output_size)\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tinput = self.word_embeddings(input_sentences)\n",
    "\t\t# input.size() = (batch_size, num_seq, embedding_length)\n",
    "\t\tinput = input.unsqueeze(1)\n",
    "\t\t# input.size() = (batch_size, 1, num_seq, embedding_length)\n",
    "\t\tmax_out1 = self.conv_block(input, self.conv1)\n",
    "\t\tmax_out2 = self.conv_block(input, self.conv2)\n",
    "\t\tmax_out3 = self.conv_block(input, self.conv3)\n",
    "\t\t\n",
    "\t\tall_out = torch.cat((max_out1, max_out2, max_out3), 1)\n",
    "\t\t# all_out.size() = (batch_size, num_kernels*out_channels)\n",
    "\t\tfc_in = self.dropout(all_out)\n",
    "\t\t# fc_in.size()) = (batch_size, num_kernels*out_channels)\n",
    "\t\tlogits = self.label(fc_in)\n",
    "\t\t\n",
    "\t\treturn logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "    \n",
    "def train_model(model, train_iter, epoch):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.cuda()\n",
    "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        if (text.size()[0] != 32):# One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        prediction = model(text)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optim.step()\n",
    "        steps += 1\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
    "\n",
    "def eval_model(model, val_iter):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.text[0]\n",
    "            if (text.size()[0] != 32):\n",
    "                continue\n",
    "            target = batch.label\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            prediction = model(text)\n",
    "            loss = loss_fn(prediction, target)\n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters setting\n",
    "learning_rate = 2e-5\n",
    "batch_size = 32 # Batch size for training and evaluation\n",
    "output_size = 2 # Number of classes for classification: 2\n",
    "in_channels = 1 # Number of input channels for CNN: 1\n",
    "out_channels = 10 # Number of output channels/feature maps for CNN\n",
    "kernel_heights = [5, 7, 9] # Kernel heights for CNN: 5, 7, 9\n",
    "stride = 1 # Stride for CNN\n",
    "padding = 0 # Padding for CNN\n",
    "keep_probab = 0.5 # Probability of retaining an activation node during dropout operation\n",
    "embedding_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Idx: 100, Training Loss: 0.6914, Training Accuracy:  59.38%\n",
      "Epoch: 1, Idx: 200, Training Loss: 0.6054, Training Accuracy:  68.75%\n",
      "Epoch: 1, Idx: 300, Training Loss: 0.6070, Training Accuracy:  68.75%\n",
      "Epoch: 1, Idx: 400, Training Loss: 0.4575, Training Accuracy:  75.00%\n",
      "Epoch: 1, Idx: 500, Training Loss: 0.5003, Training Accuracy:  78.12%\n",
      "Epoch: 01, Train Loss: 0.550, Train Acc: 70.88%, Val. Loss: 0.454949, Val. Acc: 78.66%\n",
      "Epoch: 2, Idx: 100, Training Loss: 0.5232, Training Accuracy:  75.00%\n",
      "Epoch: 2, Idx: 200, Training Loss: 0.5008, Training Accuracy:  78.12%\n",
      "Epoch: 2, Idx: 300, Training Loss: 0.6430, Training Accuracy:  68.75%\n",
      "Epoch: 2, Idx: 400, Training Loss: 0.5149, Training Accuracy:  71.88%\n",
      "Epoch: 2, Idx: 500, Training Loss: 0.4644, Training Accuracy:  71.88%\n",
      "Epoch: 02, Train Loss: 0.459, Train Acc: 78.52%, Val. Loss: 0.437626, Val. Acc: 79.52%\n",
      "Epoch: 3, Idx: 100, Training Loss: 0.3243, Training Accuracy:  90.62%\n",
      "Epoch: 3, Idx: 200, Training Loss: 0.4291, Training Accuracy:  81.25%\n",
      "Epoch: 3, Idx: 300, Training Loss: 0.4950, Training Accuracy:  78.12%\n",
      "Epoch: 3, Idx: 400, Training Loss: 0.7691, Training Accuracy:  65.62%\n",
      "Epoch: 3, Idx: 500, Training Loss: 0.4383, Training Accuracy:  81.25%\n",
      "Epoch: 03, Train Loss: 0.422, Train Acc: 80.82%, Val. Loss: 0.416021, Val. Acc: 80.74%\n",
      "Epoch: 4, Idx: 100, Training Loss: 0.4381, Training Accuracy:  84.38%\n",
      "Epoch: 4, Idx: 200, Training Loss: 0.5362, Training Accuracy:  84.38%\n",
      "Epoch: 4, Idx: 300, Training Loss: 0.2827, Training Accuracy:  84.38%\n",
      "Epoch: 4, Idx: 400, Training Loss: 0.3564, Training Accuracy:  81.25%\n",
      "Epoch: 4, Idx: 500, Training Loss: 0.2328, Training Accuracy:  93.75%\n",
      "Epoch: 04, Train Loss: 0.395, Train Acc: 82.19%, Val. Loss: 0.413711, Val. Acc: 80.85%\n",
      "Epoch: 5, Idx: 100, Training Loss: 0.3389, Training Accuracy:  84.38%\n",
      "Epoch: 5, Idx: 200, Training Loss: 0.3297, Training Accuracy:  90.62%\n",
      "Epoch: 5, Idx: 300, Training Loss: 0.3074, Training Accuracy:  84.38%\n",
      "Epoch: 5, Idx: 400, Training Loss: 0.5869, Training Accuracy:  75.00%\n",
      "Epoch: 5, Idx: 500, Training Loss: 0.2962, Training Accuracy:  87.50%\n",
      "Epoch: 05, Train Loss: 0.373, Train Acc: 83.43%, Val. Loss: 0.418192, Val. Acc: 80.78%\n",
      "Epoch: 6, Idx: 100, Training Loss: 0.5036, Training Accuracy:  81.25%\n",
      "Epoch: 6, Idx: 200, Training Loss: 0.4327, Training Accuracy:  78.12%\n",
      "Epoch: 6, Idx: 300, Training Loss: 0.3892, Training Accuracy:  81.25%\n",
      "Epoch: 6, Idx: 400, Training Loss: 0.3987, Training Accuracy:  78.12%\n",
      "Epoch: 6, Idx: 500, Training Loss: 0.3718, Training Accuracy:  81.25%\n",
      "Epoch: 06, Train Loss: 0.351, Train Acc: 84.63%, Val. Loss: 0.425157, Val. Acc: 80.44%\n",
      "Epoch: 7, Idx: 100, Training Loss: 0.4398, Training Accuracy:  87.50%\n",
      "Epoch: 7, Idx: 200, Training Loss: 0.3621, Training Accuracy:  81.25%\n",
      "Epoch: 7, Idx: 300, Training Loss: 0.2420, Training Accuracy:  93.75%\n",
      "Epoch: 7, Idx: 400, Training Loss: 0.2719, Training Accuracy:  87.50%\n",
      "Epoch: 7, Idx: 500, Training Loss: 0.3167, Training Accuracy:  87.50%\n",
      "Epoch: 07, Train Loss: 0.330, Train Acc: 85.73%, Val. Loss: 0.425317, Val. Acc: 80.53%\n",
      "Epoch: 8, Idx: 100, Training Loss: 0.2565, Training Accuracy:  87.50%\n",
      "Epoch: 8, Idx: 200, Training Loss: 0.2223, Training Accuracy:  93.75%\n",
      "Epoch: 8, Idx: 300, Training Loss: 0.2940, Training Accuracy:  81.25%\n",
      "Epoch: 8, Idx: 400, Training Loss: 0.4169, Training Accuracy:  78.12%\n",
      "Epoch: 8, Idx: 500, Training Loss: 0.3636, Training Accuracy:  84.38%\n",
      "Epoch: 08, Train Loss: 0.316, Train Acc: 86.17%, Val. Loss: 0.437247, Val. Acc: 80.33%\n",
      "Epoch: 9, Idx: 100, Training Loss: 0.4707, Training Accuracy:  81.25%\n",
      "Epoch: 9, Idx: 200, Training Loss: 0.2929, Training Accuracy:  87.50%\n",
      "Epoch: 9, Idx: 300, Training Loss: 0.5763, Training Accuracy:  75.00%\n",
      "Epoch: 9, Idx: 400, Training Loss: 0.4097, Training Accuracy:  87.50%\n",
      "Epoch: 9, Idx: 500, Training Loss: 0.1958, Training Accuracy:  93.75%\n",
      "Epoch: 09, Train Loss: 0.308, Train Acc: 86.71%, Val. Loss: 0.450381, Val. Acc: 80.44%\n",
      "Epoch: 10, Idx: 100, Training Loss: 0.2371, Training Accuracy:  90.62%\n",
      "Epoch: 10, Idx: 200, Training Loss: 0.1680, Training Accuracy:  96.88%\n",
      "Epoch: 10, Idx: 300, Training Loss: 0.4752, Training Accuracy:  78.12%\n",
      "Epoch: 10, Idx: 400, Training Loss: 0.2235, Training Accuracy:  90.62%\n",
      "Epoch: 10, Idx: 500, Training Loss: 0.3447, Training Accuracy:  84.38%\n",
      "Epoch: 10, Train Loss: 0.287, Train Acc: 87.73%, Val. Loss: 0.454012, Val. Acc: 80.31%\n"
     ]
    }
   ],
   "source": [
    "model = CNN(batch_size,\n",
    "            output_size, \n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_heights, \n",
    "            stride, \n",
    "            padding, \n",
    "            keep_probab, \n",
    "            vocab_size, \n",
    "            embedding_length, \n",
    "            weights = word_embeddings\n",
    "            )\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
    "    val_loss, val_acc = eval_model(model, valid_iter)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.442, Test Acc: 80.47%\n",
      "This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\n",
      "Sentiment: Positive\n",
      "Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\n",
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = eval_model(model, test_iter)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    with torch.no_grad():\n",
    "        test_sen = np.asarray(sentence)\n",
    "        test_sen = torch.LongTensor(test_sen)\n",
    "        test_tensor = Variable(test_sen)\n",
    "        test_tensor = test_tensor.cuda()\n",
    "        model.eval()\n",
    "        output = model(test_tensor, 1)\n",
    "        out = F.softmax(output, 1)\n",
    "        if (torch.argmax(out[0]) == 1):\n",
    "            print (\"Sentiment: Positive\")\n",
    "        else:\n",
    "            print (\"Sentiment: Negative\")\n",
    "    \n",
    "''' Let us now predict the sentiment on a single sentence just for the testing purpose. '''\n",
    "test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
    "test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n",
    "\n",
    "print(test_sen1)\n",
    "test_sen1 = TEXT.preprocess(test_sen1)\n",
    "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
    "predict_sentiment(model, test_sen1)\n",
    "\n",
    "print(test_sen2)\n",
    "test_sen2 = TEXT.preprocess(test_sen2)\n",
    "test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
    "predict_sentiment(model, test_sen2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
